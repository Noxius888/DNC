{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a simple implementation of the DNC. In this example we use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNC:\n",
    "    def __init__(self, input_size, output_size, seq_len, num_words=256, word_size=64, num_heads=4):\n",
    "        #input data\n",
    "        self.input_size = input_size #X\n",
    "        \n",
    "        #output data\n",
    "        self.output_size = output_size #Y\n",
    "        \n",
    "        #define read + write vector size\n",
    "        self.num_words = num_words #N\n",
    "        self.word_size = word_size #W\n",
    "        \n",
    "        self.num_heads = num_heads #R\n",
    "\n",
    "        #size of output vector from controller that defines interactions with memory matrix\n",
    "        self.interface_size = num_heads*word_size + 3*word_size + 5*num_heads + 3\n",
    "\n",
    "        #the actual size of the neural network input after flatenning and\n",
    "        # concatenating the input vector with the previously read vectors from memory\n",
    "        self.nn_input_size = num_heads * word_size + input_size\n",
    "        \n",
    "        #size of output\n",
    "        self.nn_output_size = output_size + self.interface_size\n",
    "        \n",
    "        #gaussian normal distribution for both outputs\n",
    "        self.nn_out = tf.truncated_normal([1, self.output_size], stddev=0.1)\n",
    "        self.interface_vec = tf.truncated_normal([1, self.interface_size], stddev=0.1)\n",
    "\n",
    "        #Create memory matrix\n",
    "        self.mem_mat = tf.zeros([num_words, word_size]) #N*W\n",
    "        \n",
    "        #other variables\n",
    "        #The usage vector records which locations have been used so far, \n",
    "        self.usage_vec = tf.fill([num_words, 1], 1e-6) #N*1\n",
    "        #a temporal link matrix records the order in which locations were written;\n",
    "        self.link_mat = tf.zeros([num_words,num_words]) #N*N\n",
    "        #represents degrees to which last location was written to\n",
    "        self.precedence_weight = tf.zeros([num_words, 1]) #N*1\n",
    "\n",
    "        #Read and write head variables\n",
    "        self.read_weights = tf.fill([num_words, num_heads], 1e-6) #N*R\n",
    "        self.write_weights = tf.fill([num_words, 1], 1e-6) #N*1\n",
    "        self.read_vecs = tf.fill([num_heads, word_size], 1e-6) #R*W\n",
    "\n",
    "        ###NETWORK VARIABLES\n",
    "        #gateways into the computation graph for input output pairs\n",
    "        self.i_data = tf.placeholder(tf.float32, [seq_len*2, self.input_size], name='input_node')\n",
    "        self.o_data = tf.placeholder(tf.float32, [seq_len*2, self.output_size], name='output_node')\n",
    "        \n",
    "        #2 layer feedforwarded network\n",
    "        self.W1 = tf.Variable(tf.truncated_normal([self.nn_input_size, 32], stddev=0.1), name='layer1_weights', dtype=tf.float32)\n",
    "        self.b1 = tf.Variable(tf.zeros([32]), name='layer1_bias', dtype=tf.float32)\n",
    "        self.W2 = tf.Variable(tf.truncated_normal([32, self.nn_output_size], stddev=0.1), name='layer2_weights', dtype=tf.float32)\n",
    "        self.b2 = tf.Variable(tf.zeros([self.nn_output_size]), name='layer2_bias', dtype=tf.float32)\n",
    "\n",
    "        ###DNC OUTPUT WEIGHTS\n",
    "        self.nn_out_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.output_size], stddev=0.1), name='net_output_weights')\n",
    "        self.interface_weights = tf.Variable(tf.truncated_normal([self.nn_output_size, self.interface_size], stddev=0.1), name='interface_weights')\n",
    "        \n",
    "        self.read_vecs_out_weight = tf.Variable(tf.truncated_normal([self.num_heads*self.word_size, self.output_size], stddev=0.1), name='read_vector_weights')\n",
    "\n",
    "    #3 attention mechanisms for read/writes to memory \n",
    "    \n",
    "    #1. key vector emitted by the controller is compared to the \n",
    "    #content of each location in memory according to a similarity measure\n",
    "    def content_lookup(self, key, str):\n",
    "        #The l2 norm of a vector is the square root of the sum of the \n",
    "        #absolute values squared\n",
    "        norm_mem = tf.nn.l2_normalize(self.mem_mat, 1) #N*W\n",
    "        norm_key = tf.nn.l2_normalize(key, 0) #1*W for write or R*W for read\n",
    "        #get similarity measure between both vectors, transpose before multiplicaiton\n",
    "        ##(N*W,W*1)->N*1 for write\n",
    "        #(N*W,W*R)->N*R for read\n",
    "        sim = tf.matmul(norm_mem, norm_key, transpose_b=True) \n",
    "        #str is 1*1 or 1*R\n",
    "        #returns similarity measure\n",
    "        return tf.nn.softmax(sim*str, 0) #N*1 or N*R\n",
    "\n",
    "    #2\n",
    "    #retreives the writing allocation weighting based on the usage free list\n",
    "    #The ‘usage’ of each location is represented as a number between 0 and 1, \n",
    "    #and a weighting that picks out unused locations is delivered to the write head. \n",
    "    \n",
    "    def allocation_weighting(self):\n",
    "        #sorted usage - the usage vector sorted ascndingly\n",
    "        #the original indices of the sorted usage vector\n",
    "        sorted_usage_vec, free_list = tf.nn.top_k(-1 * self.usage_vec, k=self.num_words)\n",
    "        sorted_usage_vec *= -1\n",
    "        cumprod = tf.cumprod(sorted_usage_vec, axis=0, exclusive=True)\n",
    "        unorder = (1-sorted_usage_vec)*cumprod\n",
    "\n",
    "        alloc_weights = tf.zeros([self.num_words])\n",
    "        I = tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
    "        \n",
    "        #for each usage vec\n",
    "        for pos, idx in enumerate(tf.unstack(free_list[0])):\n",
    "            #flatten\n",
    "            m = tf.squeeze(tf.slice(I, [idx, 0], [1, -1]))\n",
    "            #add to weight matrix\n",
    "            alloc_weights += m*unorder[0, pos]\n",
    "        #the allocation weighting for each row in memory\n",
    "        return tf.reshape(alloc_weights, [self.num_words, 1])\n",
    "\n",
    "    #at every time step the controller receives input vector from dataset and emits output vector. \n",
    "    #it also receives a set of read vectors from the memory matrix at the previous time step via \n",
    "    #the read heads. Then it emits an interface vector that defines its interactions with the memory\n",
    "    #at the current time step\n",
    "    def step_m(self, x):\n",
    "        \n",
    "        #reshape input\n",
    "        input = tf.concat([x, tf.reshape(self.read_vecs, [1, self.num_heads*self.word_size])],1)\n",
    "        \n",
    "        #forward propagation\n",
    "        l1_out = tf.matmul(input, self.W1) + self.b1\n",
    "        l1_act = tf.nn.tanh(l1_out)\n",
    "        l2_out = tf.matmul(l1_act, self.W2) + self.b2\n",
    "        l2_act = tf.nn.tanh(l2_out)\n",
    "        \n",
    "        #output vector\n",
    "        self.nn_out = tf.matmul(l2_act, self.nn_out_weights) #(1*eta+Y, eta+Y*Y)->(1*Y)\n",
    "        #interaction vector - how to interact with memory\n",
    "        self.interface_vec = tf.matmul(l2_act, self.interface_weights) #(1*eta+Y, eta+Y*eta)->(1*eta)\n",
    "        \n",
    "        \n",
    "        partition = tf.constant([[0]*(self.num_heads*self.word_size) + [1]*(self.num_heads) + [2]*(self.word_size) + [3] + \\\n",
    "                    [4]*(self.word_size) + [5]*(self.word_size) + \\\n",
    "                    [6]*(self.num_heads) + [7] + [8] + [9]*(self.num_heads*3)], dtype=tf.int32)\n",
    "\n",
    "        #convert interface vector into a set of read write vectors\n",
    "        #using tf.dynamic_partitions(Partitions interface_vec into 10 tensors using indices from partition)\n",
    "        (read_keys, read_str, write_key, write_str,\n",
    "         erase_vec, write_vec, free_gates, alloc_gate, write_gate, read_modes) = \\\n",
    "            tf.dynamic_partition(self.interface_vec, partition, 10)\n",
    "        \n",
    "        #read vectors\n",
    "        read_keys = tf.reshape(read_keys,[self.num_heads, self.word_size]) #R*W\n",
    "        read_str = 1 + tf.nn.softplus(tf.expand_dims(read_str, 0)) #1*R\n",
    "        \n",
    "        #write vectors\n",
    "        write_key = tf.expand_dims(write_key, 0) #1*W\n",
    "        #help init our write weights\n",
    "        write_str = 1 + tf.nn.softplus(tf.expand_dims(write_str, 0)) #1*1\n",
    "        erase_vec = tf.nn.sigmoid(tf.expand_dims(erase_vec, 0)) #1*W\n",
    "        write_vec = tf.expand_dims(write_vec, 0) #1*W\n",
    "        \n",
    "        #the degree to which locations at read heads will be freed\n",
    "        free_gates = tf.nn.sigmoid(tf.expand_dims(free_gates, 0)) #1*R\n",
    "        #the fraction of writing that is being allocated in a new location\n",
    "        alloc_gate = tf.nn.sigmoid(alloc_gate) #1\n",
    "        #the amount of information to be written to memory\n",
    "        write_gate = tf.nn.sigmoid(write_gate) #1\n",
    "        #the softmax distribution between the three read modes (backward, forward, lookup)\n",
    "        #The read heads can use gates called read modes to switch between content lookup \n",
    "        #using a read key and reading out locations either forwards or backwards \n",
    "        #in the order they were written.\n",
    "        read_modes = tf.nn.softmax(tf.reshape(read_modes, [3, self.num_heads])) #3*R\n",
    "        \n",
    "        #used to calculate usage vector, what's available to write to?\n",
    "        retention_vec = tf.reduce_prod(1-free_gates*self.read_weights, reduction_indices=1)\n",
    "        #used to dynamically allocate memory\n",
    "        self.usage_vec = (self.usage_vec + self.write_weights - self.usage_vec * self.write_weights) * retention_vec\n",
    "\n",
    "        ##retreives the writing allocation weighting \n",
    "        alloc_weights = self.allocation_weighting() #N*1\n",
    "        #where to write to??\n",
    "        write_lookup_weights = self.content_lookup(write_key, write_str) #N*1\n",
    "        #define our write weights now that we know how much space to allocate for them and where to write to\n",
    "        self.write_weights = write_gate*(alloc_gate*alloc_weights + (1-alloc_gate)*write_lookup_weights)\n",
    "\n",
    "        #write erase, then write to memory!\n",
    "        self.mem_mat = self.mem_mat*(1-tf.matmul(self.write_weights, erase_vec)) + tf.matmul(self.write_weights, write_vec)\n",
    "\n",
    "        #As well as writing, the controller can read from multiple locations in memory. \n",
    "        #Memory can be searched based on the content of each location, or the associative \n",
    "        #temporal links can be followed forward and backward to recall information written \n",
    "        #in sequence or in reverse. (3rd attention mechanism)\n",
    "        \n",
    "        #updates and returns the temporal link matrix for the latest write\n",
    "        #given the precedence vector and the link matrix from previous step\n",
    "        nnweight_vec = tf.matmul(self.write_weights, tf.ones([1,self.num_words])) #N*N\n",
    "        self.link_mat = (1 - nnweight_vec - tf.transpose(nnweight_vec))*self.link_mat + \\\n",
    "                        tf.matmul(self.write_weights, self.precedence_weight, transpose_b=True)\n",
    "        self.link_mat *= tf.ones([self.num_words, self.num_words]) - tf.constant(np.identity(self.num_words, dtype=np.float32))\n",
    "\n",
    "        \n",
    "        self.precedence_weight = (1-tf.reduce_sum(self.write_weights, reduction_indices=0)) * \\\n",
    "                                 self.precedence_weight + self.write_weights\n",
    "        #3 modes - forward, backward, content lookup\n",
    "        forw_w = read_modes[2]*tf.matmul(self.link_mat, self.read_weights) #(N*N,N*R)->N*R\n",
    "        look_w = read_modes[1]*self.content_lookup(read_keys, read_str) #N*R\n",
    "        back_w = read_modes[0]*tf.matmul(self.link_mat, self.read_weights, transpose_a=True) #N*R\n",
    "\n",
    "        #use them to intiialize read weights\n",
    "        self.read_weights = back_w + look_w + forw_w #N*R\n",
    "        #create read vectors by applying read weights to memory matrix\n",
    "        self.read_vecs = tf.transpose(tf.matmul(self.mem_mat, self.read_weights, transpose_a=True)) #(W*N,N*R)^T->R*W\n",
    "\n",
    "        #multiply them together\n",
    "        read_vec_mut = tf.matmul(tf.reshape(self.read_vecs, [1, self.num_heads * self.word_size]),\n",
    "                                 self.read_vecs_out_weight)  # (1*RW, RW*Y)-> (1*Y)\n",
    "        \n",
    "        #return output + read vecs product\n",
    "        return self.nn_out+read_vec_mut\n",
    "\n",
    "    #output list of numbers (one hot encoded) by running the step function\n",
    "    def run(self):\n",
    "        big_out = []\n",
    "        for t, seq in enumerate(tf.unstack(self.i_data, axis=0)):\n",
    "            seq = tf.expand_dims(seq, 0)\n",
    "            y = self.step_m(seq)\n",
    "            big_out.append(y)\n",
    "        return tf.stack(big_out, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/noxius/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "0 0.6963274\n",
      "100 0.31262892\n",
      "200 0.18256874\n",
      "300 0.11600775\n",
      "400 0.079775736\n",
      "500 0.05336862\n",
      "600 0.036565386\n",
      "700 0.028292581\n",
      "800 0.023222812\n",
      "900 0.019882746\n",
      "1000 0.01750837\n",
      "[[0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]]\n",
      "[[ -6.2617664  -5.5519695  -6.3419766  -6.517951 ]\n",
      " [ -6.0545774  -9.661787   -5.87422    -8.025218 ]\n",
      " [ -8.834529   -6.0034337  -4.891427   -9.080696 ]\n",
      " [ -5.539449  -10.372982   -6.217328   -7.8186707]\n",
      " [ -7.493229   -7.6255956  -4.938001   -8.883426 ]\n",
      " [ -3.0558138 -14.795601   -7.009693   -7.5045934]\n",
      " [ -3.6258872  -8.721846    3.6356597  -7.551636 ]\n",
      " [  2.6267128 -17.373829   -4.663477   -5.753013 ]\n",
      " [ -3.534482   -8.534973    4.1672335  -3.474731 ]\n",
      " [ -4.178026   -5.001353   -6.324995    3.4240365]\n",
      " [ -7.035433    3.926188   -4.6314425  -4.1619062]\n",
      " [-13.131819   12.820951  -22.134634   -5.124    ]]\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noxius/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main(argv=None):\n",
    "\n",
    "    #generate the input output sequences, randomly intialized\n",
    "    num_seq = 10\n",
    "    seq_len = 6\n",
    "    seq_width = 4\n",
    "    iterations = 1000\n",
    "    con = np.random.randint(0, seq_width,size=seq_len)\n",
    "    seq = np.zeros((seq_len, seq_width))\n",
    "    seq[np.arange(seq_len), con] = 1\n",
    "    end = np.asarray([[-1]*seq_width])\n",
    "    zer = np.zeros((seq_len, seq_width))\n",
    "\n",
    "    graph = tf.Graph()\n",
    "    \n",
    "    with graph.as_default():\n",
    "        #training time\n",
    "        with tf.Session() as sess:\n",
    "            #init the DNC\n",
    "            dnc = DNC(input_size=seq_width, output_size=seq_width, seq_len=seq_len, num_words=10, word_size=4, num_heads=1)\n",
    "            \n",
    "            #calculate the predicted output\n",
    "            output = tf.squeeze(dnc.run())\n",
    "            #compare prediction to reality, get loss via sigmoid cross entropy\n",
    "            loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=output, labels=dnc.o_data))\n",
    "            #use regularizers for each layer of the controller\n",
    "            regularizers = (tf.nn.l2_loss(dnc.W1) + tf.nn.l2_loss(dnc.W2) +\n",
    "                            tf.nn.l2_loss(dnc.b1) + tf.nn.l2_loss(dnc.b2))\n",
    "            #to help the loss convergence faster\n",
    "            loss += 5e-4 * regularizers\n",
    "            #optimize the entire thing (memory + controller) using gradient descent. dope\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "            \n",
    "            #initialize input output pairs\n",
    "            tf.initialize_all_variables().run()\n",
    "            final_i_data = np.concatenate((seq, zer), axis=0)\n",
    "            final_o_data = np.concatenate((zer, seq), axis=0)\n",
    "            #for each iteration\n",
    "            for i in range(0, iterations+1):\n",
    "                #feed in each input output pair\n",
    "                feed_dict = {dnc.i_data: final_i_data, dnc.o_data: final_o_data}\n",
    "                #make predictions\n",
    "                l, _, predictions = sess.run([loss, optimizer, output], feed_dict=feed_dict)\n",
    "                if i%100==0:\n",
    "                    print(i,l)\n",
    "            #print results\n",
    "            print(final_i_data)\n",
    "            print(final_o_data)\n",
    "            print(predictions)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
